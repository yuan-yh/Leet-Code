# AI
## RAG
> RAG 的核心是把外部知识检索和大语言模型的生成能力结合起来 (先查资料，再回答问题)

### 传统 LLM 的三个痛点
- 知识局限性：模型训练完之后知识就固定了，无法实时更新。RAG 可以动态检索外部知识库（最新政策、专业文献），确保回答与时俱进。

- 幻觉问题：LLM 可能编造内容。RAG 基于检索到的真实信息生成答案，大幅降低错误率。

- 可解释性不足：普通 LLM 的答案不知道从哪来的。RAG 可以标注引用来源，用户能追溯验证。

### RAG 流程图
> 原始文件 → 提取文字 → 转成向量存入数据库 → 用户提问 → 语义搜索找到最相关的片段 → 排序 → LLM 基于这些片段生成回答

1. 知识库生成
> 原始文件 → 数据提取 → Embedding 向量化 → 存入向量知识库并创建索引

- 数据提取: 把原始文件（PDF、网页、文档等）里的文字内容提取出来。

- Embedding（向量化）: 提取出的文字被转换成一串数字（向量）。语义相近的内容，数字也会相近。

- 创建索引: 向量被存入向量数据库的同时建立索引结构（图中画的是类似神经网络的节点图）。这个索引结构让后续搜索能快速找到相关内容，而不需要逐条对比。

2. 问答检索

- 检索：用户的问题也被转成向量，检索器从向量数据库中找到语义最相似的文档片段, 并自动排序 (按相关度选出 Top N, 最相关的排最前面)

- 增强：把检索结果作为上下文输入给 LLM

- 生成：LLM 基于检索到的信息生成最终答案，并标注来源


## LangChain
### Popular Questions

## MCP
### Popular Questions

## Transformer
### Popular Questions

## AI Agent
### Popular Questions

## 多模态
### Popular Questions

## Agent Workflow
### Popular Questions