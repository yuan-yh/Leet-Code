# AI
## RAG
> RAG 的核心是把外部知识检索和大语言模型的生成能力结合起来 (先查资料，再回答问题)

### 传统 LLM 的三个痛点
- 知识局限性：模型训练完之后知识就固定了，无法实时更新。RAG 可以动态检索外部知识库（最新政策、专业文献），确保回答与时俱进。

- 幻觉问题：LLM 可能编造内容。RAG 基于检索到的真实信息生成答案，大幅降低错误率。

- 可解释性不足：普通 LLM 的答案不知道从哪来的。RAG 可以标注引用来源，用户能追溯验证。

### RAG 流程图
> 原始文件 → 提取文字 → 转成向量存入数据库 → 用户提问 → 语义搜索找到最相关的片段 → 排序 → LLM 基于这些片段生成回答

1. 知识库生成
> 原始文件 → 数据提取 → Embedding 向量化 → 存入向量知识库并创建索引

- 数据提取: 把原始文件（PDF、网页、文档等）里的文字内容提取出来。

- Embedding（向量化）: 提取出的文字被转换成一串数字（向量）。语义相近的内容，数字也会相近。

- 创建索引: 向量被存入向量数据库的同时建立索引结构（图中画的是类似神经网络的节点图）。这个索引结构让后续搜索能快速找到相关内容，而不需要逐条对比。

2. 问答检索

- 检索：用户的问题也被转成向量，检索器从向量数据库中找到语义最相似的文档片段, 并自动排序 (按相关度选出 Top N, 最相关的排最前面)

- 增强：把检索结果作为上下文输入给 LLM

- 生成：LLM 基于检索到的信息生成最终答案，并标注来源

### 架构

> RAG系统由两个核心模块组成： **检索器** 和 **生成器​**

#### 生成器

四种在RAG中常用的典型生成器： Transformer模型, LSTM, 扩散模型, 和 GAN

1. Transformer (文本生成, ChatGPT)

同时关注所有上下文，生成连贯文本

它由几个部分组成：自注意力机制、前馈网络、层归一化和残差网络。生成文本时，它每一步都会看看之前所有的内容，然后预测下一个最合适的词。

**自注意力机制**: 在处理每一个词的时候，**同时（并行地）计算**它与句子中所有其他词的关联度，并根据关联度决定重点关注谁。

这带来两个关键优势：

**1. 不会"遗忘"远处的信息**——因为每个词都能直接"看到"句子中任意位置的其他词，不管距离多远。

**2. 计算效率更高**——所有词之间的关联度是并行计算的，不需要排队等前面的词处理完。

就像一个经验丰富的作家，写每一句话时都会回顾整篇文章的上下文，确保前后连贯。目前最火的 ChatGPT 就是基于 Transformer 的。

2. LSTM（长短期记忆网络, 序列数据处理）

普通 RNN 的问题是"记性不好"——句子一长就容易忘掉前面说了什么（梯度消失问题）。

LSTM 是一种特殊的循环神经网络（RNN）, 通过设计了三个"门"*选择性记忆*，按顺序生成：
- **输入门**：决定哪些新信息值得记住
- **遗忘门**：决定哪些旧信息可以丢掉
- **输出门**：决定当前要输出什么

就像你的大脑在考试时，会**选择性记忆**——重点内容牢牢记住，不重要的自动过滤掉，最后只把需要的答案写到卷子上。

3. 扩散模型 (图像/视频生成)

分两步：先把数据弄乱，再学会怎么还原，最终能从纯噪声中生成全新的数据。

- **正向过程**：往数据里一点点加噪声（杂音），直到数据变成完全随机的"雪花屏"
- **逆向过程**：训练模型学会从噪声中一步步"去噪"，最终还原出清晰的数据

4. GAN (图像/音频生成)

两个网络不断博弈，生成器越来越会造假，判别器越来越会鉴别，最终生成器能产出非常逼真的内容。

- **生成器**：负责"造假"，尽量生成逼真的数据
- **判别器**：负责"鉴别"，判断数据是真的还是假的

就像一个造假币的人和一个验钞员互相较量——造假的人不断提高技术，验钞员也不断提高鉴别能力，最终造假技术达到以假乱真的程度。


## LangChain
### Popular Questions

## MCP
### Popular Questions

## Transformer
### Popular Questions

## AI Agent
### Popular Questions

## 多模态
### Popular Questions

## Agent Workflow
### Popular Questions